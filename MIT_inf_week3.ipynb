{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Inference with Bayes' Theorem for Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Rule for Random Variables \n",
    "\n",
    "- often in real world problems, we arent given the joint distribution of 2 random variables \n",
    "    - often we can compute it by using product rule \n",
    "- for random variables X and Y, product rule is this:\n",
    "$P_{x,y} = p_{X|Y}(x|y)$ for all $x \\epsilon X, y \\epsilon Y$ such that $p_Y(y)>0$\n",
    "\n",
    "IMPORTANT TO KNOW: $p_{X|Y}(x,y) = p_Y(y)p_{X|Y}(x|y)$ where if $p_Y(y)=0$ then $p_{X,Y}(x,y)$ is 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes' Theorem for Random Variables\n",
    "\n",
    "\"posterior\" distribution: what gives/informs our \"belief\" of what X given Y = y is after observing Y take on a special value y (and given a likelihood p_Y|X and distribution for pX)\n",
    "- denotation: $p_{X|Y}(.|y)$\n",
    "- note: it is a distribution for what we are inferring\n",
    "- essentially we have an initial prob distribution for X, and as we get more info on Y's observed values, it informs and helps us update our values/beliefs abt X \n",
    "\n",
    "- Bayes' theorem for rand variables tells us how to compute posterior distribution/how to weight each possible val of rand var X after we see Y = y\n",
    "\n",
    "$$P_{X|Y}(x|y) = \\frac{p_X(x)p_{Y|X}(y|x)}{\\sum_{x'}(x')p_{Y|X}(y|x')}$$\n",
    "\n",
    "$$ \\frac{P_{X,Y}(x,y)}{P_Y(y)} = \\frac{P_X(x)P_{Y|X}{y|x}}{P_Y(y)} = \\frac{P_X(x)P_{Y|X}(y|x)}{\\sum_{x'}P_{X,Y}(x',y)}$$\n",
    "\n",
    "NOTE: posterior distribution can be undefined, only when p_X(x) = 0\n",
    "\n",
    "- computationally, is 2 step process after Y =y observation\n",
    "1. weight the score of $p_X(x)$ by factor of $p_{Y|X}(y|x)$\n",
    "\n",
    "$$\\alpha(x|y) \\triangleq p_X(x) p_{Y|X}(y|x)$$\n",
    "\n",
    "alpha is the unnormalized posterior distrib/table atp \n",
    "- this is bc when adjusting weights on the fly, we no longer guarantee that the beliefs add up to 1\n",
    "\n",
    "2. fix unnormalized posterior table by renormalizing \n",
    "\n",
    "$$p_{X|Y}(x|y)=\\frac{\\alpha(x|y)}{\\sum_{x'}\\alpha(x'|y)}=\\frac{p_X(x)p_{Y|X}(y|x)}{\\sum_{x'}p_X(x')p_{Y|X}(y|x')}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum A Posteriori (MAP) Estimation\n",
    "\n",
    "- sometimes we care about the highest posterior probability (aka the highest value x that X can take on when Y = y)\n",
    "\n",
    "- MAP estimate: value that X can take on that maximizes the posterior distribution\n",
    "    denoted by: $\\hat{x}_{MAP} (y)$\n",
    "    is equal to: $\\hat{x}_{MAP} (y) = arg max_x p_{X|Y}(x|y)$\n",
    "\n",
    "    where arg max specifies that we are finding the value x that yields highest posterior probability, as opposed to the actual highest post prob. \n",
    "\n",
    "    - can be difficult to compute given larger sets \n",
    "    - should know the number of outcomes for the random variable (will determine what the threshold is for the max) \n",
    "    - ex: if two outcomes, then MAP should have value higher than 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Independence \n",
    "\n",
    "independence: the fact that outcomes may not/do not relate to/inform the next/previous outcome \n",
    "    denoted by: $A\\bot B$\n",
    "\n",
    "- A is indep of B if probability of A and B occurring is the same as prob of A * prob B\n",
    "- written as $p_{X|Y}(x|y)=p_X(x)$ which shows that even given Y, probability of X still remains the same as if Y never happened"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernoulli and Binomial Random Variables\n",
    "\n",
    "- Bernoulli: with only two outcomes, $\\textit{can}$ be biased (as in uneven probability)\n",
    "    - prob_table = [1: p, 0: 1-p]\n",
    "    - if rand var X has this distribution, we say X ~ Bernoulli(p) (X \"has distribution\")\n",
    "- Binomial: like multiple bernoulli's, specifies a number of times event occurs and a single probability to describe that\n",
    "    - like counting # heads for n of these biased coin flips \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5        0.16666667 0.33333333]\n",
      "[[0.25       0.25      ]\n",
      " [0.08333333 0.08333333]\n",
      " [0.16666667 0.16666667]]\n",
      "[[0.25       0.25      ]\n",
      " [0.08333333 0.08333333]\n",
      " [0.16666667 0.16666667]]\n"
     ]
    }
   ],
   "source": [
    "# Independent Random Variables Exercise \n",
    "\n",
    "'''proving p_{W,I}(w,i)=p_W(w)p_I(i)'''\n",
    "\n",
    "# defining two joint tables w two variables\n",
    "prob_W_I = np.array([[1/2, 0], [0, 1/6], [0, 1/3]])\n",
    "prob_X_Y = np.array([[1/4,1/4],[1/12,1/12],[1/6,1/6]])\n",
    "\n",
    "# marginalizing the other's axes to get the singular probability tables\n",
    "prob_W = prob_W_I.sum(axis=1)\n",
    "print(prob_W)\n",
    "prob_I = prob_W_I.sum(axis=0)\n",
    "\n",
    "prob_X = prob_X_Y.sum(axis=1)\n",
    "prob_Y = prob_X_Y.sum(axis=0)\n",
    "\n",
    "# we can do matrix multiplication, compare product of the two to og WI table values\n",
    "print(np.outer(prob_W, prob_I))\n",
    "print(np.outer(prob_X, prob_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual v Pairwise Independence\n",
    "\n",
    "mutual independence: given multiple random variables with a joint probability distribution that is = to product of all indiv distributions \n",
    "\n",
    "pairwise independence: for any 2 variables, they are independent\n",
    "- however, looking at all three together (or multiple), knowing the other two DOES inform the next \n",
    "- ex: $\\oplus$ (XOR)function, where knowing x and y will inform your z, although knowing x or y alone doesnt inform z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Independence\n",
    "\n",
    "Sam as mutual dependence, but with a third random variable that they are conditioned onto \n",
    "\n",
    "if $P_{X,Y|Z}(x,y|z) = P_{X|Z}(x|z)p_{Y|Z}(y|z)$ then they are conditionally independent\n",
    "- that is, the two random variables probabilities dont change given Z\n",
    "- having maginal independence DOESNT imply conditional indep. vice versa\n",
    "\n",
    "QUESTION: is marginally independent the same as mutual independence\n",
    "\n",
    "Explaining away: a concept/thing where once we observe one explanation, our belief of a different variable may go back down \n",
    "\n",
    "QUESTION: what is the difference between something being a joint probability and independent? it seems that to find the joint probability, you use P(x,y) = p(x)*p(y) but the same is also used to prove independence of two random variables \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
